{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorwrap Performance Benchmark\n",
    "\n",
    "This notebook benchmarks vectorwrap across all supported backends with 10,000 vectors.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Requirements:**\n",
    "- `pip install \"vectorwrap[sqlite,duckdb]\" numpy matplotlib`\n",
    "- PostgreSQL with pgvector extension (optional)\n",
    "- MySQL 8.2+ (optional)\n",
    "\n",
    "**Test Dataset:**\n",
    "- 10,000 random 384-dimensional vectors (typical sentence-transformer size)\n",
    "- 1,000 query operations\n",
    "- Measures: Insert throughput, Query QPS, Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple, Dict, Any\nimport os\nimport tempfile\nfrom vectorwrap import VectorDB\n\n# Configuration\nVECTOR_DIM = 384  # Typical sentence-transformer dimension\nNUM_VECTORS = 10_000\nNUM_QUERIES = 1_000\nTOP_K = 5\n\nprint(f\"Benchmarking {NUM_VECTORS:,} vectors, {NUM_QUERIES:,} queries\")\nprint(f\"Vector dimension: {VECTOR_DIM}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate random normalized vectors (realistic embeddings)\nnp.random.seed(42)\n\ndef generate_vectors(n: int, dim: int) -> List[List[float]]:\n    \"\"\"Generate n normalized random vectors of dimension dim.\"\"\"\n    vectors = np.random.randn(n, dim).astype(np.float32)\n    # Normalize to unit length (common for embeddings)\n    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n    vectors = vectors / norms\n    return vectors.tolist()\n\n# Generate data\nprint(\"Generating test vectors...\")\ninsert_vectors = generate_vectors(NUM_VECTORS, VECTOR_DIM)\nquery_vectors = generate_vectors(NUM_QUERIES, VECTOR_DIM)\n\nprint(f\"Generated {len(insert_vectors):,} insert vectors\")\nprint(f\"Generated {len(query_vectors):,} query vectors\")\nprint(f\"Sample vector norm: {np.linalg.norm(insert_vectors[0]):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_backend(db_url: str, name: str) -> Dict[str, Any]:\n    \"\"\"Benchmark a single backend.\"\"\"\n    print(f\"\\nTesting {name}...\")\n    \n    try:\n        # Initialize database\n        db = VectorDB(db_url)\n        collection_name = \"bench_test\"\n        \n        # Create collection\n        start_time = time.time()\n        db.create_collection(collection_name, VECTOR_DIM)\n        create_time = time.time() - start_time\n        \n        # Benchmark inserts\n        print(f\"  Inserting {NUM_VECTORS:,} vectors...\")\n        start_time = time.time()\n        \n        for i, vector in enumerate(insert_vectors):\n            db.upsert(collection_name, i, vector)\n            if (i + 1) % 1000 == 0:\n                print(f\"    Inserted {i+1:,} vectors...\")\n        \n        insert_time = time.time() - start_time\n        insert_throughput = NUM_VECTORS / insert_time\n        \n        # Benchmark queries\n        print(f\"  Running {NUM_QUERIES:,} queries...\")\n        start_time = time.time()\n        \n        total_results = 0\n        for i, query_vector in enumerate(query_vectors):\n            results = db.query(collection_name, query_vector, top_k=TOP_K)\n            total_results += len(results)\n            if (i + 1) % 100 == 0:\n                print(f\"    Completed {i+1:,} queries...\")\n        \n        query_time = time.time() - start_time\n        query_qps = NUM_QUERIES / query_time\n        \n        results = {\n            'name': name,\n            'success': True,\n            'create_time': create_time,\n            'insert_time': insert_time,\n            'insert_throughput': insert_throughput,\n            'query_time': query_time,\n            'query_qps': query_qps,\n            'total_results': total_results,\n            'avg_results_per_query': total_results / NUM_QUERIES\n        }\n        \n        print(f\"  {name} completed successfully\")\n        print(f\"     Insert: {insert_throughput:.1f} vectors/sec\")\n        print(f\"     Query:  {query_qps:.1f} QPS\")\n        \n        return results\n        \n    except Exception as e:\n        print(f\"  {name} failed: {e}\")\n        return {\n            'name': name,\n            'success': False,\n            'error': str(e)\n        }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmarks\n",
    "\n",
    "We'll test all available backends. Some may fail if the database server isn't running - that's expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test configuration\nbackends_to_test = [\n    (\"sqlite:///:memory:\", \"SQLite (in-memory)\"),\n    (\"duckdb:///:memory:\", \"DuckDB (in-memory)\"),\n]\n\n# Add file-based backends\nwith tempfile.TemporaryDirectory() as tmpdir:\n    sqlite_file = os.path.join(tmpdir, \"bench.db\")\n    duckdb_file = os.path.join(tmpdir, \"bench.duckdb\")\n    \n    backends_to_test.extend([\n        (f\"sqlite:///{sqlite_file}\", \"SQLite (file)\"),\n        (f\"duckdb:///{duckdb_file}\", \"DuckDB (file)\"),\n    ])\n\n# Add network databases if available\nnetwork_backends = [\n    (\"postgresql://postgres:secret@localhost/postgres\", \"PostgreSQL + pgvector\"),\n    (\"mysql://root:secret@localhost:3306/vectordb\", \"MySQL (JSON fallback)\"),\n]\n\nprint(\"Available backends to test:\")\nfor url, name in backends_to_test + network_backends:\n    print(f\"  - {name}\")\n\nprint(\"\\nNetwork databases will be skipped if not available\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run benchmarks\nresults = []\n\n# Test local backends first\nfor db_url, name in backends_to_test:\n    result = benchmark_backend(db_url, name)\n    results.append(result)\n\n# Test network backends (may fail)\nfor db_url, name in network_backends:\n    result = benchmark_backend(db_url, name)\n    results.append(result)\n\nprint(\"\\nAll benchmarks completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter successful results\nsuccessful_results = [r for r in results if r.get('success', False)]\n\nprint(f\"Performance Results ({len(successful_results)} backends tested)\\n\")\nprint(f\"{'Backend':<25} {'Insert (vec/s)':<15} {'Query (QPS)':<12} {'Avg Results':<12}\")\nprint(\"-\" * 70)\n\nfor result in successful_results:\n    print(f\"{result['name']:<25} \"\n          f\"{result['insert_throughput']:<15.1f} \"\n          f\"{result['query_qps']:<12.1f} \"\n          f\"{result['avg_results_per_query']:<12.1f}\")\n\n# Show failed backends\nfailed_results = [r for r in results if not r.get('success', False)]\nif failed_results:\n    print(f\"\\nFailed backends ({len(failed_results)} failed):\")\n    for result in failed_results:\n        print(f\"  - {result['name']}: {result.get('error', 'Unknown error')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if successful_results:\n    # Create performance charts\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    names = [r['name'] for r in successful_results]\n    insert_throughput = [r['insert_throughput'] for r in successful_results]\n    query_qps = [r['query_qps'] for r in successful_results]\n    \n    # Insert throughput chart\n    bars1 = ax1.bar(names, insert_throughput, color='skyblue', alpha=0.7)\n    ax1.set_title('Insert Throughput (vectors/second)', fontsize=14, fontweight='bold')\n    ax1.set_ylabel('Vectors per Second')\n    ax1.tick_params(axis='x', rotation=45)\n    \n    # Add value labels on bars\n    for bar, value in zip(bars1, insert_throughput):\n        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(insert_throughput)*0.01,\n                f'{value:.0f}', ha='center', va='bottom', fontweight='bold')\n    \n    # Query QPS chart\n    bars2 = ax2.bar(names, query_qps, color='lightcoral', alpha=0.7)\n    ax2.set_title('Query Performance (QPS)', fontsize=14, fontweight='bold')\n    ax2.set_ylabel('Queries per Second')\n    ax2.tick_params(axis='x', rotation=45)\n    \n    # Add value labels on bars\n    for bar, value in zip(bars2, query_qps):\n        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(query_qps)*0.01,\n                f'{value:.0f}', ha='center', va='bottom', fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Performance summary\n    best_insert = max(successful_results, key=lambda x: x['insert_throughput'])\n    best_query = max(successful_results, key=lambda x: x['query_qps'])\n    \n    print(f\"\\nPerformance Champions:\")\n    print(f\"  Fastest Insert: {best_insert['name']} ({best_insert['insert_throughput']:.0f} vec/s)\")\n    print(f\"  Fastest Query:  {best_query['name']} ({best_query['query_qps']:.0f} QPS)\")\nelse:\n    print(\"No successful benchmarks to visualize\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Based on the benchmark results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Usage Recommendations:\\n\")\n\nif successful_results:\n    # Analyze results and provide recommendations\n    memory_backends = [r for r in successful_results if 'memory' in r['name'].lower()]\n    file_backends = [r for r in successful_results if 'file' in r['name'].lower()]\n    network_backends = [r for r in successful_results if any(db in r['name'].lower() for db in ['postgresql', 'mysql'])]\n    \n    print(\"For Development & Testing:\")\n    if memory_backends:\n        fastest_memory = max(memory_backends, key=lambda x: x['query_qps'])\n        print(f\"   → {fastest_memory['name']} - Fastest in-memory option ({fastest_memory['query_qps']:.0f} QPS)\")\n    \n    print(\"\\nFor Prototyping & Small Apps:\")\n    if file_backends:\n        fastest_file = max(file_backends, key=lambda x: x['query_qps'])\n        print(f\"   → {fastest_file['name']} - Best persistent option ({fastest_file['query_qps']:.0f} QPS)\")\n    \n    print(\"\\nFor Production:\")\n    if network_backends:\n        fastest_network = max(network_backends, key=lambda x: x['query_qps'])\n        print(f\"   → {fastest_network['name']} - Scalable production choice ({fastest_network['query_qps']:.0f} QPS)\")\n    else:\n        print(\"   → PostgreSQL + pgvector - Best for production (requires setup)\")\n        print(\"   → MySQL 8.2+ with native VECTOR - Good alternative\")\n    \n    print(\"\\nFor Analytics + Vectors:\")\n    duckdb_results = [r for r in successful_results if 'duckdb' in r['name'].lower()]\n    if duckdb_results:\n        best_duckdb = max(duckdb_results, key=lambda x: x['query_qps'])\n        print(f\"   → {best_duckdb['name']} - Combines analytics with vector search ({best_duckdb['query_qps']:.0f} QPS)\")\nelse:\n    print(\"Install vectorwrap with: `pip install \\\"vectorwrap[sqlite,duckdb]\\\"`\")\n    print(\"For production, set up PostgreSQL + pgvector or MySQL 8.2+\")\n\nprint(f\"\\nTest Configuration:\")\nprint(f\"   • Vectors: {NUM_VECTORS:,} x {VECTOR_DIM}D (normalized)\")\nprint(f\"   • Queries: {NUM_QUERIES:,} x top-{TOP_K}\")\nprint(f\"   • Machine: {os.uname().machine} / {os.uname().system}\")\n\nprint(\"\\nNote: Performance varies by dataset size, vector dimensions, and hardware.\")\nprint(\"      Run this benchmark on your target environment for accurate results.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}